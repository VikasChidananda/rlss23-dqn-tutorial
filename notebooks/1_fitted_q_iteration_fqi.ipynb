{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c09ada11-d3a6-4837-b4d0-9657106a54b5",
   "metadata": {},
   "source": [
    "# RLSS2023 - DQN Tutorial: Fitted Q Iteration (FQI)\n",
    "\n",
    "Website: https://rlsummerschool.com/\n",
    "\n",
    "Github repository: https://github.com/araffin/rlss23-dqn/\n",
    "\n",
    "Gymnasium documentation: https://gymnasium.farama.org/\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will implement the Fitted Q Iteration( (FQI) algorithm to solve the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) problem.\n",
    "This notebooks will first cover the basics for using the Gymnasium library: how to instantiate an environment, step into it and collect training data from the FQI algorithm.\n",
    "\n",
    "You will then learn how to implement step-by-step the FQI algorithm which is the predecessor of the [Deep Q-Network (DQN)](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1a9aa-5735-4614-9a76-031656397899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for autoformatting\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188798b-daf5-43a7-91ec-a7a922bc2034",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55494c-fff2-4459-87e1-e7399afd56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/araffin/rlss23-dqn/ --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2369b55-266c-4dbe-b14d-250ef7386407",
   "metadata": {},
   "source": [
    "## First steps with the Gym interface\n",
    "\n",
    "An environment that follows the [gym interface](https://gymnasium.farama.org/) is quite simple to use.\n",
    "It provides to this user mainly three methods, which have the following signature (for gym versions > 0.26):\n",
    "\n",
    "- `reset()` called at the beginning of an episode, it returns an observation and a dictionary with additional info (defaults to an empty dict)\n",
    "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether new state is a terminal state (episode is finished), whether the max number of timesteps is reached (episode is artificially finished), and additional information\n",
    "- (Optional) `render()` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `render_mode='rbg_array'` to retrieve an image of the scene).\n",
    "\n",
    "Under the hood, it also contains two useful properties:\n",
    "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
    "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
    "\n",
    "The best way to learn about [gym spaces](https://gymnasium.farama.org/api/spaces/) is to look at the [source code](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/spaces), but you need to know at least the main ones:\n",
    "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
    "```python\n",
    "# Example for using image as input:\n",
    "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "```                                       \n",
    "\n",
    "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
    "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af049724-3db9-4dec-a40c-3aa025735f00",
   "metadata": {},
   "source": [
    "## CartPole Environment\n",
    "\n",
    "For this example, we will use CartPole environment, a classic control problem.\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
    "\n",
    "Cartpole environment: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec422f-2edd-40df-8da2-c1093e1da38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Instantiate the environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f88b6-4fe2-45ec-bddd-4d83feb3a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box(4,) means that it is a Vector with 4 components\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Shape:\", env.observation_space.shape)\n",
    "# Discrete(2) means that there is two discrete actions\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414990e-2cbd-4f6f-b268-42f16a9b253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reset method is called at the beginning of an episode\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a95c8-d64a-43a3-adee-344891bbd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random action\n",
    "action = env.action_space.sample()\n",
    "print(f\"Sampled action: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311777a2-9518-496a-b1e8-0eb1af81fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step in the environment\n",
    "obs, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102a9c8-8407-4f88-b58d-4e3e5a00af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the obs is a numpy array\n",
    "# info is an empty dict for now but can contain any debugging info\n",
    "# reward is a scalar\n",
    "print(obs.shape, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fbec50-1da0-4bc4-8b88-79fa38480bfe",
   "metadata": {},
   "source": [
    "### Exercise (10 minutes): write the function to collect data\n",
    "\n",
    "This function collects an offline dataset of transitions that will be used to train a model using the FQI algorithm.\n",
    "\n",
    "See docstring of the function for what is expected as input/output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b3ff1-911b-4582-91f7-49420380eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OfflineData:\n",
    "    \"\"\"\n",
    "    A class to store transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    observations: np.ndarray\n",
    "    next_observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    terminateds: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f753ed-5f5d-4133-ab82-c84b3cfe2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env_id: str, n_steps: int = 50_000) -> OfflineData:\n",
    "    \"\"\"\n",
    "    Collect transitions using a random agent (sample action randomly).\n",
    "\n",
    "    :param env_id: The name of the environment.\n",
    "    :param n_steps: Number of steps to perform in the env.\n",
    "    :return: The collected transitions.\n",
    "    \"\"\"\n",
    "    # Create the Gym env\n",
    "    env = gym.make(env_id)\n",
    "\n",
    "    assert isinstance(env.observation_space, spaces.Box)\n",
    "    # Numpy arrays (buffers) to collect the data\n",
    "    observations = np.zeros((n_steps, *env.observation_space.shape))\n",
    "    next_observations = np.zeros((n_steps, *env.observation_space.shape))\n",
    "    # Discrete actions\n",
    "    actions = np.zeros((n_steps, 1))\n",
    "    rewards = np.zeros((n_steps,))\n",
    "    terminateds = np.zeros((n_steps,))\n",
    "\n",
    "    # Variable to know if the episode is over (done = terminated or truncated)\n",
    "    done = False\n",
    "    # Start the first episode\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO: Collect transitions for `n_steps` using\n",
    "    # a random agent (sample action uniformly)\n",
    "    # Do not forget to reset the environment if the current episode is over\n",
    "    # (done = terminated or truncated)\n",
    "    #\n",
    "    # TODO:\n",
    "    # 1. Sample a random action\n",
    "    # 2. Step in the env using this random action\n",
    "    # 3. Retrieve the new transition data (observation, reward, ...)\n",
    "    #  and update the numpy arrays (buffers)\n",
    "    # 4. Repeat until you collected `n_steps` transitions\n",
    "\n",
    "    for idx in range(n_steps):\n",
    "        # Sample a random action\n",
    "        action = env.action_space.sample()\n",
    "        # Step in the environment\n",
    "        next_obs, reward, terminated, truncated, info_ = env.step(action)\n",
    "\n",
    "        # Store the transition\n",
    "        observations[idx, :] = obs\n",
    "        next_observations[idx, :] = next_obs\n",
    "        actions[idx, :] = action\n",
    "        rewards[idx] = reward\n",
    "        # Only record true termination (timeouts/truncations are artificial terminations)\n",
    "        terminateds[idx] = terminated\n",
    "        # Update current observation\n",
    "        obs = next_obs\n",
    "        # Check if the episode is over\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Don't forget to reset the env at the end of an episode\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    return OfflineData(\n",
    "        observations,\n",
    "        next_observations,\n",
    "        actions,\n",
    "        rewards,\n",
    "        terminateds,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5d4b3-d3d6-4f82-9337-a8566c1fc707",
   "metadata": {},
   "source": [
    "Let's try the collect data method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a940-a35f-4b8c-86bb-94231c41a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "n_steps = 50_000\n",
    "# Collect transitions for n_steps\n",
    "data = collect_data(env_id=env_id, n_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0713b-63f3-40f0-8471-f3f7e3c53f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the collected data\n",
    "assert len(data.observations) == n_steps\n",
    "assert len(data.actions) == n_steps\n",
    "# Check that there are multiple episodes in the data\n",
    "assert not np.all(data.terminateds)\n",
    "assert np.any(data.terminateds)\n",
    "# Check the shape of the collected data\n",
    "if env_id == \"CartPole-v1\":\n",
    "    assert data.observations.shape == (n_steps, 4)\n",
    "    assert data.next_observations.shape == (n_steps, 4)\n",
    "assert data.actions.shape == (n_steps, 1)\n",
    "assert data.rewards.shape == (n_steps,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815ff42-3c8e-4ab5-835f-70438171751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from dqn_tutorial.fqi import save_data\n",
    "\n",
    "output_filename = Path(\"../data\") / f\"{env_id}_data\"\n",
    "# Create folder if it doesn't exist\n",
    "output_filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save collected data using numpy\n",
    "save_data(data, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff52b1a-6eb1-4284-b25f-8d59ab005ed5",
   "metadata": {},
   "source": [
    "## Fitted Q Iteration algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b251a-9ace-450b-afa9-7ee4128cdef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from sklearn import tree\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fb635-fa44-4a52-88be-1c17d5a639c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_input(\n",
    "    obs: np.ndarray,\n",
    "    actions: np.ndarray,\n",
    "    features_extractor: Optional[PolynomialFeatures] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate observation (batch_size, n_features)\n",
    "    and actions (batch_size, 1) along the feature axis.\n",
    "\n",
    "    :param obs: A batch of observations.\n",
    "    :param actions: A batch of actions.\n",
    "    :param features_extractor: Optionally a preprocessor\n",
    "        to extract features like PolynomialFeatures.\n",
    "    :return: The input for the scikit-learn model\n",
    "        (batch_size, n_features + 1)\n",
    "    \"\"\"\n",
    "    # Concatenate the observations and actions\n",
    "    # so we can predict qf(s_t, a_t)\n",
    "    model_input = np.concatenate((obs, actions), axis=1)\n",
    "    # Optionally: extract features from the input using preprocessor\n",
    "    if features_extractor is not None:\n",
    "        try:\n",
    "            model_input = features_extractor.transform(model_input)\n",
    "        except NotFittedError:\n",
    "            # First interation: fit the features_extractor\n",
    "            model_input = features_extractor.fit_transform(model_input)\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d873e-bc40-4a35-b98b-4fe9ce916aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First choose the regressor\n",
    "model_class = LinearRegression  # tree.DecisionTreeRegressor, RandomForestRegressor, ...\n",
    "# Optionally: extract features before feeding the input to the model\n",
    "features_extractor = PolynomialFeatures(degree=2)\n",
    "# features_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe29bd2-ea95-4778-b466-537a088615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_tutorial.fqi import load_data\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "output_filename = Path(\"../data\") / f\"{env_id}_data.npz\"\n",
    "render_mode = \"rgb_array\"\n",
    "\n",
    "# Create test environment\n",
    "env = gym.make(env_id, render_mode=render_mode)\n",
    "\n",
    "# Load saved transitions\n",
    "data = load_data(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26940b3e-4c9a-4632-a198-2e0ce7b12b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First iteration:\n",
    "# The target q-value is the reward obtained\n",
    "targets = data.rewards.copy()\n",
    "# Create input for current observations\n",
    "current_obs_input = create_model_input(data.observations, data.actions, features_extractor)\n",
    "# Fit the estimator for the current target\n",
    "model = model_class().fit(current_obs_input, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9544c-87f4-4872-9ae9-eef49c3a040a",
   "metadata": {},
   "source": [
    "### 1. Exercise (10 minutes): write the function to predict Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c52d1-4336-4caa-9d91-cfb641e7972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(\n",
    "    model: RegressorMixin,\n",
    "    obs: np.ndarray,\n",
    "    n_actions: int,\n",
    "    features_extractor: Optional[PolynomialFeatures] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieve the q-values for a set of observations.\n",
    "    qf(q_t, action) for all possible actions.\n",
    "\n",
    "    :param model: Q-value estimator\n",
    "    :param obs: A batch of observations\n",
    "    :param n_actions: Number of discrete actions.\n",
    "    :param features_extractor: Optionally a preprocessor\n",
    "        to extract features like PolynomialFeatures.\n",
    "    :return: The predicted q-values for the given observations\n",
    "        (batch_size, n_actions)\n",
    "    \"\"\"\n",
    "    batch_size = len(obs)\n",
    "    q_values = np.zeros((batch_size, n_actions))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # Predict q-value for each action\n",
    "    for action_idx in range(n_actions):\n",
    "        # Note: we should do one hot encoding if not using CartPole (n_actions > 2)\n",
    "        # Create a vector of size batch_size for the current action\n",
    "        actions = action_idx * np.ones((batch_size, 1))\n",
    "        # Concatenate the observations and the actions to obtain\n",
    "        # the input to the q-value estimator\n",
    "        model_input = create_model_input(obs, actions, features_extractor)\n",
    "        # Predict q-values for the given observation/action combination\n",
    "        # shape: (batch_size, 1)\n",
    "        predicted_q_values = model.predict(model_input)\n",
    "        q_values[:, action_idx] = predicted_q_values\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910968b-16db-41fe-a6d9-ff8c65322c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = 2\n",
    "n_actions = int(env.action_space.n)\n",
    "\n",
    "q_values = get_q_values(model, data.observations[:n_observations], n_actions, features_extractor)\n",
    "\n",
    "assert q_values.shape == (n_observations, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883ff38-48e9-4f7c-8692-33a9d1b325d2",
   "metadata": {},
   "source": [
    "### 2. Exercise (5 minutes): write the function to evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f4b18-a811-43fa-925d-20ad2f4d52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: RegressorMixin,\n",
    "    env: gym.Env,\n",
    "    n_eval_episodes: int = 10,\n",
    "    features_extractor: Optional[PolynomialFeatures] = None,\n",
    ") -> None:\n",
    "    episode_returns, episode_reward = [], 0.0\n",
    "    total_episodes = 0\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    assert isinstance(env.action_space, spaces.Discrete), \"FQI only support discrete actions\"\n",
    "\n",
    "    while total_episodes < n_eval_episodes:\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        # Retrieve the q-values for the current observation\n",
    "        q_values = get_q_values(\n",
    "            model,\n",
    "            obs[np.newaxis, ...],\n",
    "            int(env.action_space.n),\n",
    "            features_extractor,\n",
    "        )\n",
    "        # Select the action that maximizes the q-value for each state\n",
    "        best_action = int(np.argmax(q_values, axis=1).item())\n",
    "\n",
    "        # Send the action to the env\n",
    "        obs, reward, terminated, truncated, _ = env.step(best_action)\n",
    "\n",
    "        ### END OF YOUR CODE\n",
    "\n",
    "        episode_reward += float(reward)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            episode_returns.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "            total_episodes += 1\n",
    "            obs, _ = env.reset()\n",
    "    print(f\"Total reward = {np.mean(episode_returns):.2f} +/- {np.std(episode_returns):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e326d9-1fa7-4efd-a841-b1d9e5ef9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the first iteration\n",
    "evaluate(model, env, n_eval_episodes=10, features_extractor=features_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced782e-653d-4041-8473-2c39d12ba9a9",
   "metadata": {},
   "source": [
    "### 3. Exercise (15 minutes): the fitted Q iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbff0d8-eab0-4cec-847e-23bc1c509359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max number of iterations\n",
    "n_iterations = 50\n",
    "# How often do we evaluate the learned model\n",
    "eval_freq = 2\n",
    "# How many episodes to evaluate every eval-freq\n",
    "n_eval_episodes = 10\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "# Number of discrete actions\n",
    "n_actions = int(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c99207-4a80-4263-8b13-1818f65632e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_idx in range(n_iterations):\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # Construct TD(0) target\n",
    "    # using current model and the next observations\n",
    "    next_q_values = get_q_values(\n",
    "        model,\n",
    "        data.next_observations,\n",
    "        n_actions=n_actions,\n",
    "        features_extractor=features_extractor,\n",
    "    )\n",
    "    # Follow-greedy policy: use the action with the highest q-value\n",
    "    next_q_values = next_q_values.max(axis=1)\n",
    "    # The new target is the reward + what our agent expect to get\n",
    "    # if it follows a greedy policy (follow action with the highest q-value)\n",
    "    targets = data.rewards + gamma * (1 - data.terminateds) * next_q_values\n",
    "    # Update our q-value estimate with the current target\n",
    "    model = model_class().fit(current_obs_input, targets)\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    if (iter_idx + 1) % eval_freq == 0:\n",
    "        print(f\"Iter {iter_idx + 1}\")\n",
    "        print(f\"Score: {model.score(current_obs_input, targets):.2f}\")\n",
    "        evaluate(model, env, n_eval_episodes, features_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a60230-6477-4318-8d60-10f6eada6064",
   "metadata": {},
   "source": [
    "### Going further\n",
    "\n",
    "- play with different models/features_extractor\n",
    "- play with the discount factor\n",
    "- play with the number of data collected/used\n",
    "- combine data from random policy with data from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a3b3e-03fd-4653-b5e8-98b0ea7c6564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
