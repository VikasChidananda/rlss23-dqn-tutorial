{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c09ada11-d3a6-4837-b4d0-9657106a54b5",
   "metadata": {},
   "source": [
    "# RLSS2023 - DQN Tutorial: Deep Q-Network (DQN)\n",
    "\n",
    "Website: https://rlsummerschool.com/\n",
    "\n",
    "Github repository: https://github.com/araffin/rlss23-dqn/\n",
    "\n",
    "Gymnasium documentation: https://gymnasium.farama.org/\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will implement the [Deep Q-Network (DQN)](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) algorithm. \n",
    "It can be seed as a successor of Fitted Q Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1a9aa-5735-4614-9a76-031656397899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for autoformatting\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188798b-daf5-43a7-91ec-a7a922bc2034",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55494c-fff2-4459-87e1-e7399afd56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/araffin/rlss23-dqn/ --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2369b55-266c-4dbe-b14d-250ef7386407",
   "metadata": {},
   "source": [
    "### 1. Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34722639-6c01-4c5b-ab65-c8d5cb2adaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a796f6-393d-47cd-96f4-6856aa2fe382",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TorchReplayBufferSamples:\n",
    "    observations: th.Tensor\n",
    "    next_observations: th.Tensor\n",
    "    actions: th.Tensor\n",
    "    rewards: th.Tensor\n",
    "    terminated: th.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4111234-2d0c-4ffc-8ad0-b36619dcd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplayBufferSamples:\n",
    "    \"\"\"\n",
    "    A dataclass containing transitions from the replay buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    observations: np.ndarray\n",
    "    next_observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    terminated: np.ndarray\n",
    "\n",
    "    def to_torch(self, device: str = \"cpu\") -> TorchReplayBufferSamples:\n",
    "        \"\"\"\n",
    "        Convert the samples to PyTorch tensors.\n",
    "\n",
    "        :param device: PyTorch device\n",
    "        :return: Samples as PyTorch tensors\n",
    "        \"\"\"\n",
    "        return TorchReplayBufferSamples(\n",
    "            observations=th.as_tensor(self.observations, device=device),\n",
    "            next_observations=th.as_tensor(self.next_observations, device=device),\n",
    "            actions=th.as_tensor(self.actions, device=device),\n",
    "            rewards=th.as_tensor(self.rewards, device=device),\n",
    "            terminated=th.as_tensor(self.terminated, device=device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aafc38e2-db83-4cd9-aa26-57a04896023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple replay buffer class to store and sample transitions.\n",
    "\n",
    "    :param buffer_size: Max number of transitions to store\n",
    "    :param observation_space: Observation space of the env,\n",
    "        contains information about the observation type and shape.\n",
    "    :param action_space: Action space of the env,\n",
    "        contains information about the number of actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        observation_space: spaces.Box,\n",
    "        action_space: spaces.Discrete,\n",
    "    ) -> None:\n",
    "        self.current_idx = 0\n",
    "        self.buffer_size = buffer_size\n",
    "        self.is_full = False\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        # Create the different buffers\n",
    "        self.observations = np.zeros((buffer_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.next_observations = np.zeros((buffer_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        # The action is an integer\n",
    "        action_dim = 1\n",
    "        self.actions = np.zeros((buffer_size, action_dim), dtype=action_space.dtype)\n",
    "        self.rewards = np.zeros((buffer_size,), dtype=np.float32)\n",
    "        self.terminated = np.zeros((buffer_size,), dtype=bool)\n",
    "\n",
    "    def store_transition(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        next_obs: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Store one transition in the buffer.\n",
    "\n",
    "        :param obs: Current observation\n",
    "        :param next_obs: Next observation\n",
    "        :param action: Action taken for the current observation\n",
    "        :param reward: Reward received after taking the action\n",
    "        :param terminated: Whether it is the end of an episode or not\n",
    "            (discarding episode truncation like timeout)\n",
    "        \"\"\"\n",
    "        # Update the buffers\n",
    "        self.observations[self.current_idx] = obs\n",
    "        self.next_observations[self.current_idx] = next_obs\n",
    "        self.actions[self.current_idx] = action\n",
    "        self.rewards[self.current_idx] = reward\n",
    "        self.terminated[self.current_idx] = terminated\n",
    "        # Update the pointer, this is a ring buffer, we start from zero again when the buffer is full\n",
    "        self.current_idx += 1\n",
    "        if self.current_idx == self.buffer_size:\n",
    "            self.is_full = True\n",
    "            self.current_idx = 0\n",
    "\n",
    "    def sample(self, batch_size: int) -> ReplayBufferSamples:\n",
    "        \"\"\"\n",
    "        Sample with replacement `batch_size` transitions from the buffer.\n",
    "\n",
    "        :param batch_size: How many transitions to sample.\n",
    "        :return: Samples from the replay buffer\n",
    "        \"\"\"\n",
    "        upper_bound = self.buffer_size if self.is_full else self.current_idx\n",
    "        batch_indices = np.random.randint(0, upper_bound, size=batch_size)\n",
    "        return ReplayBufferSamples(\n",
    "            self.observations[batch_indices],\n",
    "            self.next_observations[batch_indices],\n",
    "            self.actions[batch_indices],\n",
    "            self.rewards[batch_indices],\n",
    "            self.terminated[batch_indices],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4c768-2c3b-4eac-b025-5ed04eaa241c",
   "metadata": {},
   "source": [
    "testing the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ec422f-2edd-40df-8da2-c1093e1da38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "buffer = ReplayBuffer(1000, env.observation_space, env.action_space)\n",
    "obs, _ = env.reset()\n",
    "# Fill the buffer\n",
    "for _ in range(500):\n",
    "    action = int(env.action_space.sample())\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    buffer.store_transition(obs, next_obs, action, float(reward), terminated)\n",
    "    # Update current observation\n",
    "    obs = next_obs\n",
    "\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "assert not buffer.is_full\n",
    "assert buffer.current_idx == 500\n",
    "samples = buffer.sample(batch_size=10)\n",
    "assert len(samples.observations) == 10\n",
    "assert samples.actions.shape == (10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9196c-bf2a-4433-bc36-3100a145a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the buffer completely\n",
    "for _ in range(1000):\n",
    "    action = int(env.action_space.sample())\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    buffer.store_transition(obs, next_obs, action, float(reward), terminated)\n",
    "    # Update current observation\n",
    "    obs = next_obs\n",
    "\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "assert buffer.is_full\n",
    "# We did a full loop\n",
    "assert buffer.current_idx == 500\n",
    "# Check sampling with replacement\n",
    "samples = buffer.sample(batch_size=1001)\n",
    "assert len(samples.observations) == 1001\n",
    "assert samples.actions.shape == (1001, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34619317-64d8-4872-a25e-750caa0f1588",
   "metadata": {},
   "source": [
    "### 2. Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7210df0c-4e0c-4dc0-a893-b5a1ccb815d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A Q-Network for the DQN algorithm\n",
    "    to estimate the q-value for a given observation.\n",
    "\n",
    "    :param observation_space: Observation space of the env,\n",
    "        contains information about the observation type and shape.\n",
    "    :param action_space: Action space of the env,\n",
    "        contains information about the number of actions.\n",
    "    :param n_hidden_units: Number of units for each hidden layer.\n",
    "    :param activation_fn: Activation function (ReLU by default)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Box,\n",
    "        action_space: spaces.Discrete,\n",
    "        n_hidden_units: int = 64,\n",
    "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Assume 1d space\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        # Retrieve the number of discrete actions\n",
    "        n_actions = int(action_space.n)\n",
    "        # Create the q network (2 fully connected hidden layers)\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, n_hidden_units),\n",
    "            activation_fn(),\n",
    "            nn.Linear(n_hidden_units, n_hidden_units),\n",
    "            activation_fn(),\n",
    "            nn.Linear(n_hidden_units, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        :param observations: A batch of observation (batch_size, obs_dim)\n",
    "        :return: The Q-values for the given observations\n",
    "            for all the action (batch_size, n_actions)\n",
    "        \"\"\"\n",
    "        return self.q_net(observations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba21ac-b2c3-4d23-bf5a-6ea1ffd62ec7",
   "metadata": {},
   "source": [
    "### 3. Epsilon-greedy data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f66aafe-b4eb-4878-af9a-b83b29e0d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(\n",
    "    q_net: QNetwork,\n",
    "    observation: np.ndarray,\n",
    "    exploration_rate: float,\n",
    "    action_space: spaces.Discrete,\n",
    "    device: str = \"cpu\",\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Select an action according to an espilon-greedy policy:\n",
    "    with a probability of epsilon (`exploration_rate`),\n",
    "    sample a random action, otherwise follow the best known action\n",
    "    according to the q-value.\n",
    "\n",
    "    :param observation: A single observation.\n",
    "    :param q_net: Q-network for estimating the q value\n",
    "    :param exploration_rate: Current rate of exploration (in [0, 1], 0 means no exploration),\n",
    "        probability to select a random action,\n",
    "        this is \"epsilon\".\n",
    "    :param action_space: Action space of the env,\n",
    "        contains information about the number of actions.\n",
    "    :param device: PyTorch device\n",
    "    :return: An action selected according to the epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        # Random action\n",
    "        action = int(action_space.sample())\n",
    "    else:\n",
    "        # Greedy action\n",
    "        with th.no_grad():\n",
    "            # Convert to PyTorch and add a batch dimension\n",
    "            obs_tensor = th.as_tensor(observation[np.newaxis, ...], device=device)\n",
    "            # Compute q values for all actions\n",
    "            q_values = q_net(obs_tensor)\n",
    "            # Greedy policy: select action with the highest q value\n",
    "            action = q_values.argmax().item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6823ed-8f88-48dc-a5a9-4e9d260603ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_one_step(\n",
    "    env: gym.Env,\n",
    "    q_net: QNetwork,\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    obs: np.ndarray,\n",
    "    exploration_rate: float = 0.1,\n",
    "    verbose: int = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Collect one transition and fill the replay buffer following an epsilon greedy policy.\n",
    "\n",
    "    :param env: The environment object.\n",
    "    :param q_net: Q-network for estimating the q value\n",
    "    :param replay_buffer: Replay buffer to store the new transitions.\n",
    "    :param obs: The current observation.\n",
    "    :param exploration_rate: Current rate of exploration (in [0, 1], 0 means no exploration),\n",
    "        probability to select a random action,\n",
    "        this is \"epsilon\".\n",
    "    :param verbose: The verbosity level (1 to print some info).\n",
    "    :return: The last observation (important when collecting data multiple times).\n",
    "    \"\"\"\n",
    "    assert isinstance(env.action_space, spaces.Discrete)\n",
    "\n",
    "    # Select an action following an epsilon-greedy policy\n",
    "    action = epsilon_greedy_action_selection(q_net, obs, exploration_rate, env.action_space)\n",
    "    # Step in the env\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    # Store the transition in the replay buffer\n",
    "    replay_buffer.store_transition(obs, next_obs, action, float(reward), terminated)\n",
    "    # Update current observation\n",
    "    obs = next_obs\n",
    "\n",
    "    if \"episode\" in info and verbose >= 1:\n",
    "        print(f\"Episode return={float(info['episode']['r']):.2f} length={int(info['episode']['l'])}\")\n",
    "\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        # Don't forget to reset the env at the end of an episode\n",
    "        obs, _ = env.reset()\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b251a-9ace-450b-afa9-7ee4128cdef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from sklearn import tree\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "251042a1-0250-4559-8f75-db5c7fd67d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value: float, final_value: float, current_step: int, max_steps: int) -> float:\n",
    "    \"\"\"\n",
    "    Linear schedule for the exploration rate (epsilon).\n",
    "    Note: we clip the value so the schedule is constant after reaching the final value\n",
    "    at `max_steps`.\n",
    "\n",
    "    :param initial_value: Initial value of the schedule.\n",
    "    :param final_value: Final value of the schedule.\n",
    "    :param current_step: Current step of the schedule.\n",
    "    :param max_steps: Maximum number of steps of the schedule.\n",
    "    :return: The current value of the schedule.\n",
    "    \"\"\"\n",
    "    # Compute current progress (in [0, 1], 0 being the start)\n",
    "    progress = current_step / max_steps\n",
    "    # Clip the progress so the schedule is constant after reaching the final value\n",
    "    progress = min(progress, 1.0)\n",
    "    return initial_value + progress * (final_value - initial_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82559d16-3fc6-4d47-a373-f94adcab7102",
   "metadata": {},
   "source": [
    "### 4. DQN Update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fb635-fa44-4a52-88be-1c17d5a639c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_input(\n",
    "    obs: np.ndarray,\n",
    "    actions: np.ndarray,\n",
    "    features_extractor: Optional[PolynomialFeatures] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate observation (batch_size, n_features)\n",
    "    and actions (batch_size, 1) along the feature axis.\n",
    "\n",
    "    :param obs: A batch of observations.\n",
    "    :param actions: A batch of actions.\n",
    "    :param features_extractor: Optionally a preprocessor\n",
    "        to extract features like PolynomialFeatures.\n",
    "    :return: The input for the scikit-learn model\n",
    "        (batch_size, n_features + 1)\n",
    "    \"\"\"\n",
    "    # Concatenate the observations and actions\n",
    "    # so we can predict qf(s_t, a_t)\n",
    "    model_input = np.concatenate((obs, actions), axis=1)\n",
    "    # Optionally: extract features from the input using preprocessor\n",
    "    if features_extractor is not None:\n",
    "        try:\n",
    "            model_input = features_extractor.transform(model_input)\n",
    "        except NotFittedError:\n",
    "            # First interation: fit the features_extractor\n",
    "            model_input = features_extractor.fit_transform(model_input)\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab535b42-e643-422a-8087-51311b4cdb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b269a237-a0db-40a4-9621-449538c410b7",
   "metadata": {},
   "source": [
    "### 5. DQN Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27f17152-59cf-495a-bde7-73b6f219e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_update_no_target(\n",
    "    q_net: QNetwork,\n",
    "    optimizer: th.optim.Optimizer,\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    batch_size: int,\n",
    "    gamma: float,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform one gradient step on the Q-network\n",
    "    using the data from the replay buffer.\n",
    "    Note: this is the same as dqn_update in dqn.py, but without the target network.\n",
    "\n",
    "    :param q_net: The Q-network to update\n",
    "    :param optimizer: The optimizer to use\n",
    "    :param replay_buffer: The replay buffer containing the transitions\n",
    "    :param batch_size: The minibatch size, how many transitions to sample\n",
    "    :param gamma: The discount factor\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample the replay buffer and convert them to PyTorch tensors\n",
    "    replay_data = replay_buffer.sample(batch_size).to_torch()\n",
    "\n",
    "    with th.no_grad():\n",
    "        # Compute the Q-values for the next observations (batch_size, n_actions)\n",
    "        next_q_values = q_net(replay_data.next_observations)\n",
    "        # Follow greedy policy: use the one with the highest value\n",
    "        # (batch_size,)\n",
    "        next_q_values, _ = next_q_values.max(dim=1)\n",
    "        # If the episode is terminated, set the target to the reward\n",
    "        should_bootstrap = th.logical_not(replay_data.terminated)\n",
    "        # 1-step TD target\n",
    "        td_target = replay_data.rewards + gamma * next_q_values * should_bootstrap\n",
    "\n",
    "    # Get current Q-values estimates for the replay_data (batch_size, n_actions)\n",
    "    q_values = q_net(replay_data.observations)\n",
    "    # Select the Q-values corresponding to the actions that were selected\n",
    "    # during data collection\n",
    "    current_q_values = th.gather(q_values, dim=1, index=replay_data.actions)\n",
    "    # Reshape from (batch_size, 1) to (batch_size,) to avoid broadcast error\n",
    "    current_q_values = current_q_values.squeeze(dim=1)\n",
    "\n",
    "    # Check for any shape/broadcast error\n",
    "    # Current q-values must have the same shape as the TD target\n",
    "    assert current_q_values.shape == (batch_size,), f\"{current_q_values.shape} != {(batch_size,)}\"\n",
    "    assert current_q_values.shape == td_target.shape, f\"{current_q_values.shape} != {td_target.shape}\"\n",
    "\n",
    "    # Compute the Mean Squared Error (MSE) loss\n",
    "    # Optionally, one can use a Huber loss instead of the MSE loss\n",
    "    loss = ((current_q_values - td_target) ** 2).mean()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the parameters of the q-network\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d873e-bc40-4a35-b98b-4fe9ce916aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First choose the regressor\n",
    "model_class = LinearRegression  # tree.DecisionTreeRegressor, RandomForestRegressor, ...\n",
    "# Optionally: extract features before feeding the input to the model\n",
    "features_extractor = PolynomialFeatures(degree=2)\n",
    "# features_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe29bd2-ea95-4778-b466-537a088615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_tutorial.fqi import load_data\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "output_filename = Path(\"../data\") / f\"{env_id}_data.npz\"\n",
    "render_mode = \"rgb_array\"\n",
    "\n",
    "# Create test environment\n",
    "env = gym.make(env_id, render_mode=render_mode)\n",
    "\n",
    "# Load saved transitions\n",
    "data = load_data(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26940b3e-4c9a-4632-a198-2e0ce7b12b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First iteration:\n",
    "# The target q-value is the reward obtained\n",
    "targets = data.rewards.copy()\n",
    "# Create input for current observations\n",
    "current_obs_input = create_model_input(data.observations, data.actions, features_extractor)\n",
    "# Fit the estimator for the current target\n",
    "model = model_class().fit(current_obs_input, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9544c-87f4-4872-9ae9-eef49c3a040a",
   "metadata": {},
   "source": [
    "### 1. Exercise (10 minutes): write the function to predict Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c52d1-4336-4caa-9d91-cfb641e7972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(\n",
    "    model: RegressorMixin,\n",
    "    obs: np.ndarray,\n",
    "    n_actions: int,\n",
    "    features_extractor: Optional[PolynomialFeatures] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieve the q-values for a set of observations.\n",
    "    qf(q_t, action) for all possible actions.\n",
    "\n",
    "    :param model: Q-value estimator\n",
    "    :param obs: A batch of observations\n",
    "    :param n_actions: Number of discrete actions.\n",
    "    :param features_extractor: Optionally a preprocessor\n",
    "        to extract features like PolynomialFeatures.\n",
    "    :return: The predicted q-values for the given observations\n",
    "        (batch_size, n_actions)\n",
    "    \"\"\"\n",
    "    batch_size = len(obs)\n",
    "    q_values = np.zeros((batch_size, n_actions))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # Predict q-value for each action\n",
    "    for action_idx in range(n_actions):\n",
    "        # Note: we should do one hot encoding if not using CartPole (n_actions > 2)\n",
    "        # Create a vector of size batch_size for the current action\n",
    "        actions = action_idx * np.ones((batch_size, 1))\n",
    "        # Concatenate the observations and the actions to obtain\n",
    "        # the input to the q-value estimator\n",
    "        model_input = create_model_input(obs, actions, features_extractor)\n",
    "        # Predict q-values for the given observation/action combination\n",
    "        # shape: (batch_size, 1)\n",
    "        predicted_q_values = model.predict(model_input)\n",
    "        q_values[:, action_idx] = predicted_q_values\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910968b-16db-41fe-a6d9-ff8c65322c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = 2\n",
    "n_actions = int(env.action_space.n)\n",
    "\n",
    "q_values = get_q_values(model, data.observations[:n_observations], n_actions, features_extractor)\n",
    "\n",
    "assert q_values.shape == (n_observations, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883ff38-48e9-4f7c-8692-33a9d1b325d2",
   "metadata": {},
   "source": [
    "### 2. Exercise (5 minutes): write the function to evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f4b18-a811-43fa-925d-20ad2f4d52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: RegressorMixin,\n",
    "    env: gym.Env,\n",
    "    n_eval_episodes: int = 10,\n",
    "    features_extractor: Optional[PolynomialFeatures] = None,\n",
    ") -> None:\n",
    "    episode_returns, episode_reward = [], 0.0\n",
    "    total_episodes = 0\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    assert isinstance(env.action_space, spaces.Discrete), \"FQI only support discrete actions\"\n",
    "\n",
    "    while total_episodes < n_eval_episodes:\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        # Retrieve the q-values for the current observation\n",
    "        q_values = get_q_values(\n",
    "            model,\n",
    "            obs[np.newaxis, ...],\n",
    "            int(env.action_space.n),\n",
    "            features_extractor,\n",
    "        )\n",
    "        # Select the action that maximizes the q-value for each state\n",
    "        best_action = int(np.argmax(q_values, axis=1).item())\n",
    "\n",
    "        # Send the action to the env\n",
    "        obs, reward, terminated, truncated, _ = env.step(best_action)\n",
    "\n",
    "        ### END OF YOUR CODE\n",
    "\n",
    "        episode_reward += float(reward)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            episode_returns.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "            total_episodes += 1\n",
    "            obs, _ = env.reset()\n",
    "    print(f\"Total reward = {np.mean(episode_returns):.2f} +/- {np.std(episode_returns):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e326d9-1fa7-4efd-a841-b1d9e5ef9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the first iteration\n",
    "evaluate(model, env, n_eval_episodes=10, features_extractor=features_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced782e-653d-4041-8473-2c39d12ba9a9",
   "metadata": {},
   "source": [
    "### 3. Exercise (15 minutes): the fitted Q iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbff0d8-eab0-4cec-847e-23bc1c509359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max number of iterations\n",
    "n_iterations = 50\n",
    "# How often do we evaluate the learned model\n",
    "eval_freq = 2\n",
    "# How many episodes to evaluate every eval-freq\n",
    "n_eval_episodes = 10\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "# Number of discrete actions\n",
    "n_actions = int(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c99207-4a80-4263-8b13-1818f65632e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_idx in range(n_iterations):\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # Construct TD(0) target\n",
    "    # using current model and the next observations\n",
    "    next_q_values = get_q_values(\n",
    "        model,\n",
    "        data.next_observations,\n",
    "        n_actions=n_actions,\n",
    "        features_extractor=features_extractor,\n",
    "    )\n",
    "    # Follow-greedy policy: use the action with the highest q-value\n",
    "    next_q_values = next_q_values.max(axis=1)\n",
    "    # The new target is the reward + what our agent expect to get\n",
    "    # if it follows a greedy policy (follow action with the highest q-value)\n",
    "    targets = data.rewards + gamma * (1 - data.terminateds) * next_q_values\n",
    "    # Update our q-value estimate with the current target\n",
    "    model = model_class().fit(current_obs_input, targets)\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    if (iter_idx + 1) % eval_freq == 0:\n",
    "        print(f\"Iter {iter_idx + 1}\")\n",
    "        print(f\"Score: {model.score(current_obs_input, targets):.2f}\")\n",
    "        evaluate(model, env, n_eval_episodes, features_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a60230-6477-4318-8d60-10f6eada6064",
   "metadata": {},
   "source": [
    "### Going further\n",
    "\n",
    "- play with different models/features_extractor\n",
    "- play with the discount factor\n",
    "- play with the number of data collected/used\n",
    "- combine data from random policy with data from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a3b3e-03fd-4653-b5e8-98b0ea7c6564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
